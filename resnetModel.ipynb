{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60083c7b-bfaf-48a4-86f5-fb69976c95a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.11), please consider upgrading to the latest version (0.3.13).\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/nih-chest-xrays/data?dataset_version_number=3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 42.0G/42.0G [23:54<00:00, 31.4MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset root: /Users/graysonrichard/.cache/kagglehub/datasets/nih-chest-xrays/data/versions/3\n",
      "Immediate contents of dataset_root:\n",
      "['images_006', 'images_001', 'images_008', 'images_009', 'images_007', 'FAQ_CHESTXRAY.pdf', 'images_012', 'Data_Entry_2017.csv', 'BBox_List_2017.csv', 'ARXIV_V5_CHESTXRAY.pdf', 'train_val_list.txt', 'README_CHESTXRAY.pdf', 'images_002', 'images_005', 'LOG_CHESTXRAY.pdf', 'images_004', 'images_003', 'test_list.txt', 'images_010', 'images_011']\n"
     ]
    }
   ],
   "source": [
    "import kagglehub, os\n",
    "\n",
    "# This downloads (or reuses cached) NIH Chest X-rays dataset and gives you the local path\n",
    "dataset_root = kagglehub.dataset_download(\"nih-chest-xrays/data\")\n",
    "\n",
    "print(\"Dataset root:\", dataset_root)\n",
    "print(\"Immediate contents of dataset_root:\")\n",
    "print(os.listdir(dataset_root))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29648a76-c282-4aa9-b3c6-1323d16febb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# If needed (first time in this environment) you can install:\n",
    "# !pip install torch torchvision torchaudio scikit-learn\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc5f6843-b5cc-4fa3-8674-ec10f8ae7534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV shape: (112120, 12)\n",
      "Columns: ['Image Index', 'Finding Labels', 'Follow-up #', 'Patient ID', 'Patient Age', 'Patient Gender', 'View Position', 'OriginalImage[Width', 'Height]', 'OriginalImagePixelSpacing[x', 'y]', 'Unnamed: 11']\n",
      "        Image Index          Finding Labels  Follow-up #  Patient ID  \\\n",
      "0  00000001_000.png            Cardiomegaly            0           1   \n",
      "1  00000001_001.png  Cardiomegaly|Emphysema            1           1   \n",
      "2  00000001_002.png   Cardiomegaly|Effusion            2           1   \n",
      "3  00000002_000.png              No Finding            0           2   \n",
      "4  00000003_000.png                  Hernia            0           3   \n",
      "\n",
      "   Patient Age Patient Gender View Position  OriginalImage[Width  Height]  \\\n",
      "0           58              M            PA                 2682     2749   \n",
      "1           58              M            PA                 2894     2729   \n",
      "2           58              M            PA                 2500     2048   \n",
      "3           81              M            PA                 2500     2048   \n",
      "4           81              F            PA                 2582     2991   \n",
      "\n",
      "   OriginalImagePixelSpacing[x     y]  Unnamed: 11  \n",
      "0                        0.143  0.143          NaN  \n",
      "1                        0.143  0.143          NaN  \n",
      "2                        0.168  0.168          NaN  \n",
      "3                        0.171  0.171          NaN  \n",
      "4                        0.143  0.143          NaN  \n",
      "\n",
      "Target value counts (0 = No Finding, 1 = Any finding):\n",
      "target\n",
      "0    60361\n",
      "1    51759\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Target proportion:\n",
      "target\n",
      "0    0.538361\n",
      "1    0.461639\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# dataset_root is already defined from your previous cell\n",
    "csv_path = os.path.join(dataset_root, \"Data_Entry_2017.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"CSV shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(df.head())\n",
    "\n",
    "# Option A: Binary target\n",
    "# 0 = No Finding, 1 = Has at least one finding\n",
    "df[\"target\"] = (df[\"Finding Labels\"] != \"No Finding\").astype(int)\n",
    "\n",
    "print(\"\\nTarget value counts (0 = No Finding, 1 = Any finding):\")\n",
    "print(df[\"target\"].value_counts())\n",
    "print(\"\\nTarget proportion:\")\n",
    "print(df[\"target\"].value_counts(normalize=True))\n",
    "\n",
    "# OPTIONAL: if you want to prototype faster, uncomment to subsample\n",
    "# df = df.sample(n=20000, random_state=SEED).reset_index(drop=True)\n",
    "# print(\"Subsampled CSV shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6c8925d-d7e1-477a-a5ef-a8c3df1d37e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 66590\n",
      "Val size: 22701\n",
      "Test size: 22829\n",
      "\n",
      "Train target distribution:\n",
      "target\n",
      "0    0.540201\n",
      "1    0.459799\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "patients = df[\"Patient ID\"].unique()\n",
    "\n",
    "train_patients, temp_patients = train_test_split(\n",
    "    patients, test_size=0.4, random_state=SEED\n",
    ")\n",
    "val_patients, test_patients = train_test_split(\n",
    "    temp_patients, test_size=0.5, random_state=SEED\n",
    ")\n",
    "\n",
    "def assign_split(pid):\n",
    "    if pid in train_patients:\n",
    "        return \"train\"\n",
    "    elif pid in val_patients:\n",
    "        return \"val\"\n",
    "    else:\n",
    "        return \"test\"\n",
    "\n",
    "df[\"split\"] = df[\"Patient ID\"].apply(assign_split)\n",
    "\n",
    "train_df = df[df[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "val_df   = df[df[\"split\"] == \"val\"].reset_index(drop=True)\n",
    "test_df  = df[df[\"split\"] == \"test\"].reset_index(drop=True)\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Val size:\",   len(val_df))\n",
    "print(\"Test size:\",  len(test_df))\n",
    "\n",
    "print(\"\\nTrain target distribution:\")\n",
    "print(train_df[\"target\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57681692-4917-4c0d-a1e2-7120ffc78ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images found: 112120\n",
      "Proportion of CSV rows with a matching file: 1.0000\n"
     ]
    }
   ],
   "source": [
    "image_path_map = {}\n",
    "\n",
    "for root, dirs, files in os.walk(dataset_root):\n",
    "    for fname in files:\n",
    "        if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "            full_path = os.path.join(root, fname)\n",
    "            image_path_map[fname] = full_path\n",
    "\n",
    "print(\"Number of images found:\", len(image_path_map))\n",
    "\n",
    "coverage = df[\"Image Index\"].isin(image_path_map.keys()).mean()\n",
    "print(f\"Proportion of CSV rows with a matching file: {coverage:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25444bbf-0da9-4350-a3fc-949f0e5f344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Transforms for ResNet\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # ImageNet stats\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, df, image_path_map, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.image_path_map = image_path_map\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_name = row[\"Image Index\"]\n",
    "        \n",
    "        if img_name not in self.image_path_map:\n",
    "            raise FileNotFoundError(f\"Image file not found for {img_name}\")\n",
    "        \n",
    "        img_path = self.image_path_map[img_name]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = int(row[\"target\"])  # 0 or 1\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7129738b-5695-4859-af14-b37f8a3cfb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2081, 710, 714)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 0  # keep 0 on Mac / Jupyter to avoid multiprocessing issues\n",
    "\n",
    "train_dataset = ChestXrayDataset(train_df, image_path_map, transform=train_transform)\n",
    "val_dataset   = ChestXrayDataset(val_df,   image_path_map, transform=eval_transform)\n",
    "test_dataset  = ChestXrayDataset(test_df,  image_path_map, transform=eval_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if device.type == \"cuda\" else False,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if device.type == \"cuda\" else False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True if device.type == \"cuda\" else False,\n",
    ")\n",
    "\n",
    "len(train_loader), len(val_loader), len(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2eb0504-44b4-48f3-ae00-f8c2257f3282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /Users/graysonrichard/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:02<00:00, 18.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "num_feats = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_feats, 1)\n",
    "\n",
    "resnet = resnet.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a279c53e-7917-4110-96af-a2203f159885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for images, labels in tqdm(loader, desc=\"Train\", leave=False):\n",
    "        images = images.to(device)\n",
    "        labels = labels.float().unsqueeze(1).to(device)  # (N, 1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def eval_one_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Eval\", leave=False):\n",
    "            images = images.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            probs = torch.sigmoid(outputs).cpu().numpy().flatten()\n",
    "            all_probs.extend(probs)\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "    \n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    all_labels = np.array(all_labels, dtype=np.float32)\n",
    "    all_probs = np.array(all_probs, dtype=np.float32)\n",
    "    \n",
    "    return epoch_loss, all_labels, all_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2cf4d0c5-8a55-42e0-b68c-0c6f4f85d20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 112120\n",
      "Smaller balanced dataset size: 6000\n",
      "target\n",
      "0    3000\n",
      "1    3000\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gn/8m537hpn0vgggtr35s7vfr380000gn/T/ipykernel_13339/1132200546.py:5: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(min(len(x), N_PER_CLASS), random_state=SEED))\n"
     ]
    }
   ],
   "source": [
    "N_PER_CLASS = 3000  # total ~6000 images; bump to e.g. 5000 for ~10k\n",
    "\n",
    "df_small = (\n",
    "    df.groupby(\"target\", group_keys=False)\n",
    "      .apply(lambda x: x.sample(min(len(x), N_PER_CLASS), random_state=SEED))\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"Original dataset size:\", len(df))\n",
    "print(\"Smaller balanced dataset size:\", len(df_small))\n",
    "print(df_small[\"target\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aef8246e-2e93-4af4-b59f-72d16f25536e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 3615\n",
      "Val size: 1183\n",
      "Test size: 1202\n",
      "\n",
      "Train target distribution:\n",
      "target\n",
      "1    0.505671\n",
      "0    0.494329\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "patients = df_small[\"Patient ID\"].unique()\n",
    "\n",
    "train_patients, temp_patients = train_test_split(\n",
    "    patients, test_size=0.4, random_state=SEED\n",
    ")\n",
    "val_patients, test_patients = train_test_split(\n",
    "    temp_patients, test_size=0.5, random_state=SEED\n",
    ")\n",
    "\n",
    "def assign_split(pid):\n",
    "    if pid in train_patients:\n",
    "        return \"train\"\n",
    "    elif pid in val_patients:\n",
    "        return \"val\"\n",
    "    else:\n",
    "        return \"test\"\n",
    "\n",
    "df_small[\"split\"] = df_small[\"Patient ID\"].apply(assign_split)\n",
    "\n",
    "train_df = df_small[df_small[\"split\"] == \"train\"].reset_index(drop=True)\n",
    "val_df   = df_small[df_small[\"split\"] == \"val\"].reset_index(drop=True)\n",
    "test_df  = df_small[df_small[\"split\"] == \"test\"].reset_index(drop=True)\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Val size:\",   len(val_df))\n",
    "print(\"Test size:\",  len(test_df))\n",
    "print(\"\\nTrain target distribution:\")\n",
    "print(train_df[\"target\"].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce5e87fe-cedb-467e-b8ec-074b00788d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113, 37, 38)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, df, image_path_map, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.image_path_map = image_path_map\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_name = row[\"Image Index\"]\n",
    "        \n",
    "        img_path = self.image_path_map[img_name]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = int(row[\"target\"])\n",
    "        return img, label\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 0  # keep 0 on Mac / notebooks\n",
    "\n",
    "train_dataset = ChestXrayDataset(train_df, image_path_map, transform=train_transform)\n",
    "val_dataset   = ChestXrayDataset(val_df,   image_path_map, transform=eval_transform)\n",
    "test_dataset  = ChestXrayDataset(test_df,  image_path_map, transform=eval_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS)\n",
    "\n",
    "len(train_loader), len(val_loader), len(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bcf167ce-d2e3-48a4-8212-14dc4a25ffb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Epoch 1/2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6470 | Val loss: 0.6622\n",
      "ðŸ‘‰ New best model (saved in memory)\n",
      "\n",
      "===== Epoch 2/2 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5636 | Val loss: 0.6722\n",
      "\n",
      "Loaded best model based on validation loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "NUM_EPOCHS = 2  # start with 2; bump to 3â€“5 if it's still okay\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_state_dict = None\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n===== Epoch {epoch+1}/{NUM_EPOCHS} =====\")\n",
    "    \n",
    "    train_loss = train_one_epoch(resnet, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_y_true, val_y_prob = eval_one_epoch(resnet, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"Train loss: {train_loss:.4f} | Val loss: {val_loss:.4f}\")\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_state_dict = resnet.state_dict()\n",
    "        print(\"ðŸ‘‰ New best model (saved in memory)\")\n",
    "\n",
    "if best_state_dict is not None:\n",
    "    resnet.load_state_dict(best_state_dict)\n",
    "    print(\"\\nLoaded best model based on validation loss.\")\n",
    "else:\n",
    "    print(\"\\nWARNING: No best_state_dict saved; using last-epoch model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "79d58b0a-9714-4b2e-95f4-ebe52022b155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 0.6382\n",
      "Number of test samples: 1202\n",
      "First 10 true labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "First 10 predicted probabilities: [0.39562777 0.42700183 0.40202335 0.19539285 0.50387543 0.26866668\n",
      " 0.34148452 0.39076948 0.29376507 0.60493916]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_true</th>\n",
       "      <th>y_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.395628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.427002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.402023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.195393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.503875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y_true    y_prob\n",
       "0     0.0  0.395628\n",
       "1     0.0  0.427002\n",
       "2     0.0  0.402023\n",
       "3     0.0  0.195393\n",
       "4     0.0  0.503875"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_y_true, test_y_prob = eval_one_epoch(\n",
    "    resnet, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(f\"\\nTest loss: {test_loss:.4f}\")\n",
    "print(\"Number of test samples:\", len(test_y_true))\n",
    "\n",
    "print(\"First 10 true labels:\", test_y_true[:10])\n",
    "print(\"First 10 predicted probabilities:\", test_y_prob[:10])\n",
    "\n",
    "# Save results in a DataFrame for further analysis (confusion matrix, subgroups, etc.)\n",
    "test_results_df = pd.DataFrame({\n",
    "    \"y_true\": test_y_true,\n",
    "    \"y_prob\": test_y_prob,\n",
    "})\n",
    "\n",
    "test_results_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e9d1af3-067b-4945-8fd6-6d0f449b2488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL EVAL AND DATA EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "50a45371-7e30-4926-b2c5-7689aff73b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (rows = true, cols = pred):\n",
      "[[489 121]\n",
      " [311 281]]\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Finding       0.61      0.80      0.69       610\n",
      " Any Finding       0.70      0.47      0.57       592\n",
      "\n",
      "    accuracy                           0.64      1202\n",
      "   macro avg       0.66      0.64      0.63      1202\n",
      "weighted avg       0.65      0.64      0.63      1202\n",
      "\n",
      "ROC-AUC: 0.7159\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "# Threshold at 0.5 (you could tune this later if you want)\n",
    "test_y_pred = (test_y_prob >= 0.5).astype(int)\n",
    "\n",
    "cm = confusion_matrix(test_y_true.astype(int), test_y_pred)\n",
    "print(\"Confusion matrix (rows = true, cols = pred):\")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(\n",
    "    test_y_true.astype(int),\n",
    "    test_y_pred,\n",
    "    target_names=[\"No Finding\", \"Any Finding\"]\n",
    "))\n",
    "\n",
    "# ROC-AUC is nice to report too\n",
    "try:\n",
    "    auc = roc_auc_score(test_y_true, test_y_prob)\n",
    "    print(f\"ROC-AUC: {auc:.4f}\")\n",
    "except ValueError:\n",
    "    print(\"Could not compute ROC-AUC (only one class present?).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "049fd693-5542-4c2b-9008-3af3a387c6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 0.6382\n",
      "Number of test samples: 1202\n",
      "First 10 true labels: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "First 10 predicted probabilities: [0.39562777 0.42700183 0.40202335 0.19539285 0.50387543 0.26866668\n",
      " 0.34148452 0.39076948 0.29376507 0.60493916]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Evaluate best model on test set\n",
    "test_loss, test_y_true, test_y_prob = eval_one_epoch(\n",
    "    resnet, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "print(f\"\\nTest loss: {test_loss:.4f}\")\n",
    "print(\"Number of test samples:\", len(test_y_true))\n",
    "print(\"First 10 true labels:\", test_y_true[:10])\n",
    "print(\"First 10 predicted probabilities:\", test_y_prob[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef41563f-370d-40d2-82da-c9629c51bc18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
